{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "cecbt62xZjYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install razdel rouge transformers sentencepiece --quiet"
      ],
      "metadata": {
        "id": "7LI-oSjeB_62"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from razdel import sentenize\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import tarfile\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "id": "GcJOSwiQDU5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f gazeta_raw.txt\n",
        "!wget https://www.dropbox.com/s/4fxj5wmt7tjr5f2/gazeta_raw.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEsJlyrLDcGO",
        "outputId": "c013ae57-f882-4d70-bca0-05810b48e683"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-20 16:39:53--  https://www.dropbox.com/s/4fxj5wmt7tjr5f2/gazeta_raw.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/y8n5zf87bxpw1ob1pbkfl/gazeta_raw.txt?rlkey=zjbt0qur4f11svazg2d8ackba [following]\n",
            "--2025-10-20 16:39:54--  https://www.dropbox.com/scl/fi/y8n5zf87bxpw1ob1pbkfl/gazeta_raw.txt?rlkey=zjbt0qur4f11svazg2d8ackba\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com/cd/0/inline/CzlZrRsqUVn68WywQ9KH1oieMWHrnlZmmDDObJ_AeciHe8If6W6WLH92NHO5GY5crUyeNQFvPN4xn4tCAUIll3rNREUBMY9eCfdgsvveL5cZlcF2Tl93p8FLb1j4RfqlDQ0/file# [following]\n",
            "--2025-10-20 16:39:54--  https://ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com/cd/0/inline/CzlZrRsqUVn68WywQ9KH1oieMWHrnlZmmDDObJ_AeciHe8If6W6WLH92NHO5GY5crUyeNQFvPN4xn4tCAUIll3rNREUBMY9eCfdgsvveL5cZlcF2Tl93p8FLb1j4RfqlDQ0/file\n",
            "Resolving ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com (ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com (ucdbb8d2d221397e1c75213d5b33.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 704863137 (672M) [text/plain]\n",
            "Saving to: ‘gazeta_raw.txt’\n",
            "\n",
            "gazeta_raw.txt      100%[===================>] 672.21M  33.8MB/s    in 15s     \n",
            "\n",
            "2025-10-20 16:40:10 (43.9 MB/s) - ‘gazeta_raw.txt’ saved [704863137/704863137]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive summarization"
      ],
      "metadata": {
        "id": "7v2UQJtSZ0b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
      ],
      "metadata": {
        "id": "ALMqA_px2zr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8d8a1dce-8d4c-4155-e08a-eda6c7a16ada"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f0fe8c5c-9c87-4256-9c0c-24e59757a9e9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/adapter_config.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f0fe8c5c-9c87-4256-9c0c-24e59757a9e9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/adapter_config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary(text, top_n=3, limit=300):\n",
        "    sentences = [s.text for s in sentenize(text)]\n",
        "    if len(sentences) <= top_n:\n",
        "        return \" \".join(sentences)[:limit]\n",
        "\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "\n",
        "    # sematic centroid\n",
        "    centroid = np.mean(sentence_embeddings, axis=0, keepdims=True)\n",
        "    centroid_similarity = cosine_similarity(sentence_embeddings, centroid).ravel()\n",
        "\n",
        "    # positional weight\n",
        "    positional_weight = np.array([1.0 - (i * 0.8 / len(sentences)) for i in range(len(sentences))])\n",
        "\n",
        "    # combine\n",
        "    combined_scores = (\n",
        "        0.7 * centroid_similarity +\n",
        "        0.3 * positional_weight\n",
        "    )\n",
        "\n",
        "    # get top 3 sentences with heighest score\n",
        "    top_indices = np.argsort(combined_scores)[-top_n:]\n",
        "    selected_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "    summary = \" \".join(selected_sentences)\n",
        "\n",
        "    # cut to limit\n",
        "    if len(summary) > limit:\n",
        "        sentences_in_summary = [s.text for s in sentenize(summary)]\n",
        "        current_length = 0\n",
        "        final_sentences = []\n",
        "\n",
        "        for sent in sentences_in_summary:\n",
        "            if current_length + len(sent) + 1 <= limit:\n",
        "                final_sentences.append(sent)\n",
        "                current_length += len(sent) + 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        summary = \" \".join(final_sentences)\n",
        "\n",
        "    if not summary.strip():\n",
        "      summary = text[:300]\n",
        "\n",
        "    return summary.strip()\n"
      ],
      "metadata": {
        "id": "nor5HJ2tGk23"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstarct summarization"
      ],
      "metadata": {
        "id": "CASOAmolasYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
        "model2 = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPzeH5Jea4hU",
        "outputId": "9c1ac4dc-320f-48ea-a8b3-9d5c26f5080d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def abstract_summary(text, max_len=300):\n",
        "    WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "    if not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    input_text = \"summarize: \" + text.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "    input_ids = tokenizer(\n",
        "      [WHITESPACE_HANDLER(text)],\n",
        "      return_tensors=\"pt\",\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      max_length=512\n",
        "      )[\"input_ids\"]\n",
        "\n",
        "    output_ids = model2.generate(\n",
        "      input_ids=input_ids,\n",
        "      max_length=84,\n",
        "      no_repeat_ngram_size=2,\n",
        "      num_beams=4\n",
        "      )[0]\n",
        "\n",
        "    summary = tokenizer.decode(\n",
        "        output_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "    if not summary.strip():\n",
        "      summary = text[:300]\n",
        "\n",
        "    return summary[:max_len]\n"
      ],
      "metadata": {
        "id": "TiJHkfE9a9hs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "hLGt4J46bh7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rouge_2(references, hypotheses):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "    return scores['rouge-2']\n"
      ],
      "metadata": {
        "id": "XXgC5b2JOdrp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_gazeta_data(input_path,\n",
        "                        output_texts_path,\n",
        "                        output_refs_path,\n",
        "                        max_samples=100,\n",
        "                        summary_limit=300):\n",
        "\n",
        "    records = []\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as r:\n",
        "        for line in r:\n",
        "            records.append(json.loads(line))\n",
        "\n",
        "\n",
        "    filtered = [\n",
        "        rec for rec in records\n",
        "        if 'text' in rec and 'summary' in rec and len(rec['summary']) < summary_limit\n",
        "    ]\n",
        "\n",
        "    filtered = filtered[:max_samples]\n",
        "    texts = [rec['text'] for rec in filtered]\n",
        "    references = [rec['summary'] for rec in filtered]\n",
        "\n",
        "    with open(output_texts_path, \"w\", encoding=\"utf-8\") as f_texts:\n",
        "        json.dump(texts, f_texts, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(output_refs_path, \"w\", encoding=\"utf-8\") as f_refs:\n",
        "        json.dump(references, f_refs, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Всего записей в исходном файле: {len(records)}\")\n",
        "    print(f\"Отобрано записей (саммари < {summary_limit}): {len(filtered)}\")\n",
        "    print(f\"Сохранено {len(texts)} текстов в: {output_texts_path}\")\n",
        "    print(f\"Сохранено {len(references)} эталонных саммари в: {output_refs_path}\")\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "t1fE4riHdLhR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summarizers(texts_path,\n",
        "                         refs_path,\n",
        "                         abstract_func,\n",
        "                         extract_func,\n",
        "                         sample_examples = 3):\n",
        "\n",
        "    with open(texts_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        texts = json.load(f)\n",
        "    with open(refs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        references = json.load(f)\n",
        "\n",
        "    assert len(texts) == len(references), \"Кол-во текстов и summary не совпадает\"\n",
        "\n",
        "    print(f\"Загружено {len(texts)} примеров\")\n",
        "\n",
        "\n",
        "    abstract_hypotheses = []\n",
        "    extract_hypotheses = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        print(f\"Обработка {i+1}/{len(texts)}\")\n",
        "        abstract_hypotheses.append(abstract_func(text))\n",
        "        extract_hypotheses.append(extract_func(text))\n",
        "\n",
        "\n",
        "    rouge = Rouge()\n",
        "\n",
        "    abs_scores = rouge.get_scores(abstract_hypotheses, references, avg=True)['rouge-2']\n",
        "    ext_scores = rouge.get_scores(extract_hypotheses, references, avg=True)['rouge-2']\n",
        "\n",
        "    print(\"Средние метрики ROUGE-2:\")\n",
        "    print(f\"Абстрактная модель:\")\n",
        "    print(f\"  Precision: {abs_scores['p']:.4f}\")\n",
        "    print(f\"  Recall:    {abs_scores['r']:.4f}\")\n",
        "    print(f\"  F1:        {abs_scores['f']:.4f}\")\n",
        "\n",
        "    print(f\"Экстрактная модель:\")\n",
        "    print(f\"  Precision: {ext_scores['p']:.4f}\")\n",
        "    print(f\"  Recall:    {ext_scores['r']:.4f}\")\n",
        "    print(f\"  F1:        {ext_scores['f']:.4f}\")\n",
        "\n",
        "    print(\"Примеры:\")\n",
        "    for i in range(min(sample_examples, len(texts))):\n",
        "        print(f\"--- Пример {i+1} ---\")\n",
        "        print(f\"Исходный текст (первые 200 символов): {texts[i][:200]}\")\n",
        "        print(f\"Эталонное summary:{references[i]}\")\n",
        "        print(f\"Абстрактное summary:{abstract_hypotheses[i]}\")\n",
        "        print(f\"Экстрактное summary:{extract_hypotheses[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    return abs_scores, ext_scores\n"
      ],
      "metadata": {
        "id": "HMqJI5MDeT_7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = 'gazeta_raw.txt'\n",
        "output_texts_path = '/content/texts.json'\n",
        "output_refs_path = '/content/references.json'\n",
        "prepare_gazeta_data(input_path, output_texts_path, output_refs_path, max_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR4wjTVVe6_y",
        "outputId": "c3bc0877-0ab2-4009-f9b8-077e0f8c75a1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего записей в исходном файле: 75198\n",
            "Отобрано записей (саммари < 300): 10\n",
            "Сохранено 10 текстов в: /content/texts.json\n",
            "Сохранено 10 эталонных саммари в: /content/references.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_scores, ext_scores = evaluate_summarizers(\n",
        "    texts_path=\"texts.json\",\n",
        "    refs_path=\"references.json\",\n",
        "    abstract_func=abstract_summary,\n",
        "    extract_func=extract_summary,\n",
        "    sample_examples=3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZFVcncye4kK",
        "outputId": "84f834b1-b4f6-4775-ab25-c8002faa5493"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружено 10 примеров.\n",
            "\n",
            "Обработка 1/10\n",
            "Обработка 2/10\n",
            "Обработка 3/10\n",
            "Обработка 4/10\n",
            "Обработка 5/10\n",
            "Обработка 6/10\n",
            "Обработка 7/10\n",
            "Обработка 8/10\n",
            "Обработка 9/10\n",
            "Обработка 10/10\n",
            "Средние метрики ROUGE-2:\n",
            "Абстрактная модель:\n",
            "  Precision: 0.0000\n",
            "  Recall:    0.0000\n",
            "  F1:        0.0000\n",
            "Экстрактная модель:\n",
            "  Precision: 0.0147\n",
            "  Recall:    0.0178\n",
            "  F1:        0.0161\n",
            "Примеры:\n",
            "--- Пример 1 ---\n",
            "Исходный текст (первые 200 символов): «У меня больше нет друзей», «Я хочу умереть», «Это страшный сон» — подобными сообщениями буквально кишат сегодня форумы и чаты, в которых пользователи делятся впечатлениями о проблемах с «аськой». Пер\n",
            "Эталонное summary:Популярный интернет-пейджер ICQ потерпел глобальный сбой — значительная часть пользователей по всему миру вдруг потеряла данные о своих «друзьях». На диверсию против ICQ происшедшее не похоже, хотя служба технической поддержки компании ICQ пока молчит.\n",
            "Абстрактное summary:Интернет-пейджер ICQ, который считается одним из крупнейших в мире интернет-сервисов, оказался в центре глобального сбоя.\n",
            "Экстрактное summary:Сначала интернет-пейджеры перестали работать, через некоторое время работа восстановилась. Но затем, по первоначальным грубым прикидкам, 20-30% пользователей ICQ (всего их 120 млн) остались без контакт-листов (списков людей, с которыми пользователь поддерживает постоянную связь по ICQ).\n",
            "--------------------\n",
            "--- Пример 2 ---\n",
            "Исходный текст (первые 200 символов): Мне самому 24 года, сам с 8 лет я общаюсь с компьютерами и до сих пор моя жизнь с ними связана - я работаю программистом, получаю около 1500 долларов в месяц, и могу с большой ответственностью заявить\n",
            "Эталонное summary:Самое главное не паникуйте.\n",
            "Абстрактное summary:Компьютеры - это инструмент, который может быть как телевизор, так и игровая приствка, а также способ восстребовать инстинкт человека к управлению.\n",
            "Экстрактное summary:Многие родители «демонизируют компьютеры» и в этом их глубокая ошибка. Стоит ли беспокоится - да, но не тому что вот компьютер - демон, а ваш мальчик типа попал под дурное влияние - это не правильно.\n",
            "--------------------\n",
            "--- Пример 3 ---\n",
            "Исходный текст (первые 200 символов): Всемирная организация здравоохранения ( ВОЗ ) выступила в среду с призывом воздержаться от поездок в Гонконг в связи со вспышкой атипичной пневмонии в странах юго-восточной Азии. Как сообщает агентств\n",
            "Эталонное summary:ВОЗ призвала жителей Земли воздержаться от поездок в Гонконг и китайскую провинцию Гуандун, где бушует смертельная инфекция, убившая уже 75 человек во всем мире. Тем временем гонконгцы пытаются изолировать всех больных, а американцы отзывают из КНР дипломатов.\n",
            "Абстрактное summary:Число заболевших атипичной пневмонией в Китае достигло 75 человек, еще около 2300 находятся под карантином.\n",
            "Экстрактное summary:Всемирная организация здравоохранения ( ВОЗ ) выступила в среду с призывом воздержаться от поездок в Гонконг в связи со вспышкой атипичной пневмонии в странах юго-восточной Азии. В другие очаги инфекции, такие как Сингапур, Таиланд и Вьетнам, по мнению медиков, летать пока не очень опасно.\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}