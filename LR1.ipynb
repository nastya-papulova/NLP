{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2627dba",
      "metadata": {
        "id": "d2627dba"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b3a61e",
      "metadata": {
        "id": "c5b3a61e"
      },
      "source": [
        "# Dictionary parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a961bc53",
      "metadata": {
        "id": "a961bc53"
      },
      "outputs": [],
      "source": [
        "dict_path = r'/content/dict.opcorpora.xml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912fd94a",
      "metadata": {
        "id": "912fd94a"
      },
      "outputs": [],
      "source": [
        "def parse_dict(xml_file_path):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    return root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0469ffde",
      "metadata": {
        "id": "0469ffde"
      },
      "outputs": [],
      "source": [
        "dict_root = parse_dict(dict_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31597af0",
      "metadata": {
        "id": "31597af0"
      },
      "source": [
        "# Parsing lemmata from dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f04bf7",
      "metadata": {
        "id": "f6f04bf7"
      },
      "outputs": [],
      "source": [
        "def parse_lemmata(root):\n",
        "    lemmata_dict = defaultdict(list)\n",
        "    lemmata_section = root.find('lemmata')\n",
        "\n",
        "    for lemma_elem in lemmata_section.findall('lemma'):\n",
        "        l_elem = lemma_elem.find('l')\n",
        "        lemma_text = l_elem.get('t', '')\n",
        "\n",
        "        first_g_elem = l_elem.find('g')\n",
        "        main_grammem = first_g_elem.get('v', '') if first_g_elem is not None else ''\n",
        "\n",
        "        for f_elem in lemma_elem.findall('f'):\n",
        "            form_text = f_elem.get('t', '')\n",
        "            if form_text:\n",
        "                new_entry = (lemma_text, main_grammem)\n",
        "\n",
        "                if new_entry not in lemmata_dict[form_text]:\n",
        "                    lemmata_dict[form_text].append(new_entry)\n",
        "\n",
        "    return dict(lemmata_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a46115a",
      "metadata": {
        "id": "6a46115a"
      },
      "outputs": [],
      "source": [
        "lemmata_dict = parse_lemmata(dict_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse universal dictionatru and get statistic"
      ],
      "metadata": {
        "id": "JgzWiQFSvvtw"
      },
      "id": "JgzWiQFSvvtw"
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_conllu(file_path):\n",
        "    sentences = []\n",
        "    current = []\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current:\n",
        "                    sentences.append(current)\n",
        "                    current = []\n",
        "                continue\n",
        "            if line.startswith(\"#\"):  # skip comment lines\n",
        "                continue\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) != 10:\n",
        "                continue\n",
        "            idx, form, lemma, upos, xpos, feats, head, deprel, deps, misc = parts\n",
        "            feats_dict = {}\n",
        "            if feats != \"_\":\n",
        "                for feat in feats.split(\"|\"):\n",
        "                    k, v = feat.split(\"=\")\n",
        "                    feats_dict[k] = v\n",
        "            current.append({\n",
        "                \"form\": form,\n",
        "                \"lemma\": lemma,\n",
        "                \"upos\": upos,\n",
        "                \"feats\": feats_dict\n",
        "            })\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "o_q9be9YBQYw"
      },
      "id": "o_q9be9YBQYw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bayes_stats(sentences, window=1):\n",
        "    emission_counts = defaultdict(Counter)  # word -> POS -> count\n",
        "    emission_totals = Counter()\n",
        "    context_counts = defaultdict(Counter)  # context_POS -> POS -> count\n",
        "    context_totals = Counter()\n",
        "\n",
        "    for sent in sentences:\n",
        "        upos_seq = [normalize_tag(tok[\"upos\"]) for tok in sent]\n",
        "        forms = [tok[\"form\"].lower() for tok in sent]\n",
        "\n",
        "        for i, (w, g_raw) in enumerate(zip(forms, upos_seq)):\n",
        "            g = g_raw\n",
        "            emission_counts[w][g] += 1\n",
        "            emission_totals[w] += 1\n",
        "\n",
        "            for j in range(max(0, i - window), min(len(sent), i + window + 1)):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                ctx = upos_seq[j]\n",
        "                context_counts[ctx][g] += 1\n",
        "                context_totals[ctx] += 1\n",
        "\n",
        "    return emission_counts, emission_totals, context_counts, context_totals\n"
      ],
      "metadata": {
        "id": "MTtJK2fOBQR6"
      },
      "id": "MTtJK2fOBQR6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = parse_conllu(\"/content/ru_gsd-ud-train.conllu\")"
      ],
      "metadata": {
        "id": "U-1vSxjdBTXA"
      },
      "id": "U-1vSxjdBTXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add synthetic sentences to balance\n",
        "synthetic = [\n",
        "    {\"form\": \"Я\", \"lemma\": \"я\", \"upos\": \"PRON\"},\n",
        "    {\"form\": \"люблю\", \"lemma\": \"любить\", \"upos\": \"VERB\"},\n",
        "    {\"form\": \"печь\", \"lemma\": \"печь\", \"upos\": \"VERB\"},\n",
        "    {\"form\": \"пироги\", \"lemma\": \"пирог\", \"upos\": \"NOUN\"}\n",
        "]\n",
        "sentences.append(synthetic)\n"
      ],
      "metadata": {
        "id": "x9uJ9eGZmy7L"
      },
      "id": "x9uJ9eGZmy7L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emission_counts, emission_totals, context_counts, context_totals = build_bayes_stats(sentences)\n",
        "all_tags = list({g for s in sentences for g in [tok[\"upos\"] for tok in s]})\n"
      ],
      "metadata": {
        "id": "DkVaJRQ_m0Qe"
      },
      "id": "DkVaJRQ_m0Qe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(emission_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrLEPJZLmhRM",
        "outputId": "44d7c2b8-6731-472b-ec8e-90081ce2abe4"
      },
      "id": "TrLEPJZLmhRM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24526"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_MAP = {\n",
        "    \"CCONJ\": \"CONJ\", \"CONJ\": \"CONJ\",\n",
        "    \"ADJF\": \"ADJ\", \"ADJ\": \"ADJ\",\n",
        "    \"ADVB\": \"ADV\", \"ADV\": \"ADV\",\n",
        "    \"INFN\": \"VERB\", \"VERB\": \"VERB\",\n",
        "    \"NOUN\": \"NOUN\", \"PROPN\": \"NOUN\",\n",
        "    \"PRCL\": \"PART\", \"PART\": \"PART\",\n",
        "    \"NPRO\": \"PRON\", \"PRON\": \"PRON\",\n",
        "    \"NUMR\": \"NUM\", \"NUM\": \"NUM\",\n",
        "    \"ADP\": \"ADP\", \"PREP\": \"ADP\",\n",
        "    \"INTJ\": \"INTJ\",\n",
        "    \"PUNCT\": \"PUNCT\",\n",
        "    \"SYM\": \"SYM\",\n",
        "}\n",
        "\n",
        "def normalize_tag(tag):\n",
        "    return TAG_MAP.get(tag, tag)"
      ],
      "metadata": {
        "id": "UkaLHfJwET6c"
      },
      "id": "UkaLHfJwET6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute global prior P(g)\n",
        "def compute_global_prior(emission_counts, emission_totals, all_tags):\n",
        "    gcount = Counter()\n",
        "    for w, c in emission_counts.items():\n",
        "        for tag, cnt in c.items():\n",
        "            gcount[normalize_tag(tag)] += cnt\n",
        "    total = sum(gcount.values())\n",
        "    if total == 0:\n",
        "        return {t: 1.0/len(all_tags) for t in all_tags}\n",
        "    return {t: gcount.get(t, 0)/total for t in all_tags}\n",
        "\n",
        "EPS = 1e-12\n",
        "\n",
        "# P(g|word)\n",
        "def P_g_given_w(word, g, emission_counts, emission_totals, all_tags, smooth=1.0):\n",
        "    word = word.lower()\n",
        "    g = normalize_tag(g)\n",
        "    if word not in emission_counts:\n",
        "        return 0.0\n",
        "    denom = emission_totals.get(word, 0) + smooth * len(all_tags)\n",
        "    return (emission_counts[word].get(g, 0) + smooth) / denom\n",
        "\n",
        "# P(g|context_tag)\n",
        "def P_g_given_context(g, context_g, context_counts, context_totals, all_tags, smooth=1.0):\n",
        "    g = normalize_tag(g)\n",
        "    ctx = normalize_tag(context_g)\n",
        "    if ctx not in context_counts:\n",
        "        return 1.0 / len(all_tags)\n",
        "    denom = context_totals.get(ctx, 0) + smooth * len(all_tags)\n",
        "    return (context_counts[ctx].get(g, 0) + smooth) / denom\n"
      ],
      "metadata": {
        "id": "i2eWyVwEwcLQ"
      },
      "id": "i2eWyVwEwcLQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def disambiguate_bayes(word, candidates, left_ctx, right_ctx,\n",
        "                       emission_counts, emission_totals,\n",
        "                       context_counts, context_totals, all_tags,\n",
        "                       alpha=0.6, beta_left=1.2, beta_right=2.8, smoothing=1e-3,\n",
        "                       global_prior=None, debug=False):\n",
        "    word_l = word.lower()\n",
        "    if global_prior is None:\n",
        "        gp = compute_global_prior(emission_counts, emission_totals, all_tags)\n",
        "    else:\n",
        "        gp = global_prior\n",
        "\n",
        "    best = None\n",
        "    best_score = -1e18\n",
        "    scores = []\n",
        "\n",
        "    for lemma, pos in candidates:\n",
        "        pos_norm = normalize_tag(pos)\n",
        "\n",
        "        # P(g|w)\n",
        "        p_em = P_g_given_w(word_l, pos_norm, emission_counts, emission_totals, all_tags, smooth=smoothing)\n",
        "\n",
        "        # backoff\n",
        "        p_comb = alpha * p_em + (1.0 - alpha) * gp.get(pos_norm, 1.0/len(all_tags))\n",
        "        # safety\n",
        "        score = math.log(p_comb + EPS)\n",
        "\n",
        "        if left_ctx:\n",
        "            p_left = P_g_given_context(pos_norm, left_ctx, context_counts, context_totals, all_tags, smooth=smoothing)\n",
        "            score += beta_left * math.log(p_left + EPS)\n",
        "\n",
        "        if right_ctx:\n",
        "            p_right = P_g_given_context(pos_norm, right_ctx, context_counts, context_totals, all_tags, smooth=smoothing)\n",
        "            score += beta_right * math.log(p_right + EPS)\n",
        "\n",
        "        scores.append(((lemma, pos_norm), score))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best = (lemma, pos_norm)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Bayes scores for\", word, \"left:\", left_ctx, \"right:\", right_ctx)\n",
        "        for (lem, p), sc in scores:\n",
        "            print(f\"  {lem}/{p}: {sc:.4f}\")\n",
        "        print(\"=> chosen:\", best, \"score\", best_score)\n",
        "\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "53H4o3kPFbKX"
      },
      "id": "53H4o3kPFbKX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_unknown_word(word, is_sentence_start=False, prev_pos=None):\n",
        "    word_lower = word.lower()\n",
        "    if word[0].isupper() and not is_sentence_start:\n",
        "        return word_lower, 'NOUN'\n",
        "    if word_lower.endswith(('ость', 'ство', 'ация', 'изм')):\n",
        "        return word_lower, normalize_tag('NOUN')\n",
        "    elif word_lower.endswith(('ый', 'ий', 'ой', 'ая', 'яя', 'ое')):\n",
        "        return word_lower, normalize_tag('ADJF')\n",
        "    elif word_lower.endswith(('ть', 'ти', 'чь')):\n",
        "        return word_lower, normalize_tag('INFN')\n",
        "    elif word_lower.endswith(('о', 'е', 'и')):\n",
        "        return word_lower, normalize_tag('ADVB')\n",
        "    elif any(char.isdigit() for char in word):\n",
        "        return word_lower, normalize_tag('NUMR')\n",
        "    return word_lower, 'NOUN'\n"
      ],
      "metadata": {
        "id": "eTqomjnkGQ_s"
      },
      "id": "eTqomjnkGQ_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cc3960a5",
      "metadata": {
        "id": "cc3960a5"
      },
      "source": [
        "# Tokenize and lemmatize text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dc6551",
      "metadata": {
        "id": "75dc6551"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.findall(r'\\w+', text)\n",
        "    return [token for token in tokens if token]\n",
        "\n",
        "def normalize(word):\n",
        "    return word.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_lemmatization(input_text, lemmata_dict,\n",
        "                                    emission_counts, emission_totals,\n",
        "                                    context_counts, context_totals,\n",
        "                                    all_tags,\n",
        "                                    alpha=0.6, beta_left=1.2, beta_right=2.8, smoothing=1e-3,\n",
        "                                    debug=False):\n",
        "    lines = input_text.strip().split('\\n')\n",
        "    results = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "          continue\n",
        "\n",
        "        tokens = tokenize(line)\n",
        "        normalized_tokens = [normalize(token) for token in tokens]\n",
        "\n",
        "        word_infos = []\n",
        "        for i, (token, normalized) in enumerate(zip(tokens, normalized_tokens)):\n",
        "            is_sentence_start = (i == 0)\n",
        "            prev_pos = word_infos[i-1]['pos'] if i > 0 else None\n",
        "\n",
        "            if normalized in lemmata_dict:\n",
        "                possible_lemmas_raw = lemmata_dict[normalized]\n",
        "                possible_lemmas = [(lem, normalize_tag(pos)) for (lem,pos) in possible_lemmas_raw]\n",
        "                has_ambiguity = len(possible_lemmas) > 1\n",
        "                lemma, pos = possible_lemmas[0]\n",
        "                word_infos.append({\n",
        "                    'token': token,\n",
        "                    'normalized': normalized,\n",
        "                    'lemma': lemma,\n",
        "                    'pos': pos,\n",
        "                    'has_ambiguity': has_ambiguity,\n",
        "                    'possible_lemmas': possible_lemmas\n",
        "                })\n",
        "            else:\n",
        "                lemma, pos = guess_unknown_word(token, is_sentence_start, prev_pos)\n",
        "                pos = normalize_tag(pos)\n",
        "                word_infos.append({\n",
        "                    'token': token,\n",
        "                    'normalized': normalized,\n",
        "                    'lemma': lemma,\n",
        "                    'pos': pos,\n",
        "                    'has_ambiguity': False,\n",
        "                    'possible_lemmas': [(lemma, pos)]\n",
        "                })\n",
        "\n",
        "        if not word_infos:  # empty line\n",
        "          continue\n",
        "        # right context\n",
        "        for i in range(len(word_infos)-1):\n",
        "            word_infos[i]['right_pos'] = word_infos[i+1]['pos']\n",
        "        word_infos[-1]['right_pos'] = None\n",
        "\n",
        "        for i, info in enumerate(word_infos):\n",
        "            if not info['has_ambiguity']:\n",
        "                continue\n",
        "            left_ctx = word_infos[i-1]['pos'] if i > 0 else None\n",
        "            right_ctx = info.get('right_pos', None)\n",
        "\n",
        "            lemma, pos = disambiguate_bayes(\n",
        "                info['normalized'],\n",
        "                info['possible_lemmas'],\n",
        "                left_ctx,\n",
        "                right_ctx,\n",
        "                emission_counts,\n",
        "                emission_totals,\n",
        "                context_counts,\n",
        "                context_totals,\n",
        "                all_tags,\n",
        "                alpha=alpha, beta_left=beta_left, beta_right=beta_right, smoothing=smoothing,\n",
        "                global_prior=None, debug=debug\n",
        "            )\n",
        "            info['lemma'] = lemma\n",
        "            info['pos'] = pos\n",
        "\n",
        "        processed_tokens = [\n",
        "            f\"{info['token']}{{{info['lemma']}={info['pos']}}}\"\n",
        "            for info in word_infos\n",
        "        ]\n",
        "        results.append(' '.join(processed_tokens))\n",
        "\n",
        "    return '\\n'.join(results)\n"
      ],
      "metadata": {
        "id": "8smjtHVJGVdT"
      },
      "id": "8smjtHVJGVdT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(process_text_with_lemmatization(\"Русская печь. я люблю печь пироги. Ваза из стекла. Вода стекла по оконной раме\",\n",
        "    lemmata_dict, emission_counts, emission_totals, context_counts, context_totals, all_tags,\n",
        "    alpha=0.7, beta_left=1.5, beta_right=2.0, smoothing=1e-4, debug=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBt-sTXIkipP",
        "outputId": "f4745ba9-3d1f-4df7-85cd-4445e1d8b3b0"
      },
      "id": "OBt-sTXIkipP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayes scores for печь left: ADJ right: PRON\n",
            "  печь/VERB: -7.7508\n",
            "  печь/NOUN: -6.3176\n",
            "=> chosen: ('печь', 'NOUN') score -6.317599555666266\n",
            "Bayes scores for печь left: VERB right: NOUN\n",
            "  печь/VERB: -10.4340\n",
            "  печь/NOUN: -5.5396\n",
            "=> chosen: ('печь', 'NOUN') score -5.539568543123753\n",
            "Bayes scores for пироги left: NOUN right: NOUN\n",
            "  пирог/NOUN: -5.4718\n",
            "  пирога/NOUN: -5.4718\n",
            "=> chosen: ('пирог', 'NOUN') score -5.471807897897936\n",
            "Bayes scores for ваза left: NOUN right: ADP\n",
            "  ваза/NOUN: -5.9691\n",
            "  ваз/NOUN: -5.9691\n",
            "=> chosen: ('ваза', 'NOUN') score -5.969063522741976\n",
            "Bayes scores for из left: NOUN right: NOUN\n",
            "  из/ADP: -6.9845\n",
            "  иза/NOUN: -7.5308\n",
            "=> chosen: ('из', 'ADP') score -6.984512065803482\n",
            "Bayes scores for стекла left: ADP right: NOUN\n",
            "  стекло/NOUN: -4.3005\n",
            "  стёк/VERB: -11.7619\n",
            "=> chosen: ('стекло', 'NOUN') score -4.300531267460037\n",
            "Bayes scores for вода left: NOUN right: NOUN\n",
            "  вод/NOUN: -5.4708\n",
            "  вода/NOUN: -5.4708\n",
            "=> chosen: ('вод', 'NOUN') score -5.470827404495894\n",
            "Bayes scores for стекла left: NOUN right: ADP\n",
            "  стекло/NOUN: -3.9101\n",
            "  стёк/VERB: -11.5112\n",
            "=> chosen: ('стекло', 'NOUN') score -3.9101057239807373\n",
            "Bayes scores for по left: NOUN right: ADJ\n",
            "  по/ADP: -7.2030\n",
            "  по/NOUN: -5.8128\n",
            "=> chosen: ('по', 'NOUN') score -5.81278097140112\n",
            "Bayes scores for раме left: ADJ right: None\n",
            "  рам/NOUN: -3.2637\n",
            "  рама/NOUN: -3.2637\n",
            "=> chosen: ('рам', 'NOUN') score -3.263703908238508\n",
            "Русская{русский=ADJ} печь{печь=NOUN} я{я=PRON} люблю{люблю=VERB} печь{печь=NOUN} пироги{пирог=NOUN} Ваза{ваза=NOUN} из{из=ADP} стекла{стекло=NOUN} Вода{вод=NOUN} стекла{стекло=NOUN} по{по=NOUN} оконной{оконный=ADJ} раме{рам=NOUN}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accuracy"
      ],
      "metadata": {
        "id": "HHMQRfRcpQD4"
      },
      "id": "HHMQRfRcpQD4"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/сrime_and_punishment.txt\", encoding=\"utf-8\") as f:\n",
        "    big_text = f.read()\n",
        "\n",
        "paragraphs = [p.strip() for p in big_text.split('\\n') if p.strip()]\n",
        "print(f\"Количество абзацев: {len(paragraphs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-KcYxWrpPcn",
        "outputId": "252f69a0-a2ee-445c-a279-644fb25a0e6d"
      },
      "id": "r-KcYxWrpPcn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество абзацев: 4879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBlk_sMTrpOK",
        "outputId": "527d9063-4509-4b35-8ce6-f9766ed01bc0"
      },
      "id": "kBlk_sMTrpOK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def analyze_with_pymorphy(text):\n",
        "    tokens = tokenize(text)\n",
        "    res = []\n",
        "    for t in tokens:\n",
        "        parse = morph.parse(t)[0]\n",
        "        res.append((t, parse.normal_form, parse.tag.POS))\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "69u8CvZRroJp"
      },
      "id": "69u8CvZRroJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P2U_TAGS = {\n",
        "    \"ADJF\": \"ADJ\",\n",
        "    \"ADJS\": \"ADJ\",\n",
        "    \"COMP\": \"ADJ\",\n",
        "    \"VERB\": \"VERB\",\n",
        "    \"INFN\": \"VERB\",\n",
        "    \"PRTF\": \"VERB\",\n",
        "    \"PRTS\": \"VERB\",\n",
        "    \"GRND\": \"VERB\",\n",
        "    \"NOUN\": \"NOUN\",\n",
        "    \"NPRO\": \"PRON\",\n",
        "    \"NUMR\": \"NUM\",\n",
        "    \"ADVB\": \"ADV\",\n",
        "    \"PREP\": \"ADP\",\n",
        "    \"CONJ\": \"CONJ\",\n",
        "    \"PRCL\": \"PART\",\n",
        "    \"INTJ\": \"INTJ\",\n",
        "    \"PRED\": \"ADV\",\n",
        "}\n",
        "\n",
        "def normalize_pymorphy_tag(tag):\n",
        "    return P2U_TAGS.get(tag, tag)\n"
      ],
      "metadata": {
        "id": "XoeB7Ew_trXn"
      },
      "id": "XoeB7Ew_trXn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(our_tags, pymorphy_tags):\n",
        "    mapping = {\n",
        "        \"ADJF\": \"ADJF\", \"ADJS\": \"ADJF\", \"COMP\": \"ADJF\",\n",
        "        \"INFN\": \"VERB\", \"VERB\": \"VERB\", \"PRTF\": \"VERB\", \"PRTS\": \"VERB\", \"GRND\": \"VERB\",\n",
        "        \"NOUN\": \"NOUN\", \"PROPN\": \"NOUN\", \"NPRO\": \"PRON\",\n",
        "        \"ADVB\": \"ADVB\", \"PREP\": \"ADP\", \"CONJ\": \"CONJ\", \"PRCL\": \"PART\",\n",
        "        \"INTJ\": \"INTJ\", \"NUMR\": \"NUM\", \"PRED\": \"ADJF\"\n",
        "    }\n",
        "\n",
        "    def normalize_tag(tag):\n",
        "        return mapping.get(tag, tag)\n",
        "\n",
        "    total = min(len(our_tags), len(pymorphy_tags))\n",
        "    correct = 0\n",
        "\n",
        "    for ours, pym in zip(our_tags[:total], pymorphy_tags[:total]):\n",
        "        w1, t1 = ours[0], ours[1]\n",
        "        w2, t2 = pym[0], pym[1]\n",
        "\n",
        "        if w1.lower() != w2.lower():\n",
        "            continue\n",
        "        if t1 == normalize_tag(t2):\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total if total else 0.0\n"
      ],
      "metadata": {
        "id": "dI1UB0yvtTEh"
      },
      "id": "dI1UB0yvtTEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \" \".join(paragraphs)\n",
        "our = process_text_with_lemmatization(sample_text, lemmata_dict,\n",
        "                                      emission_counts, emission_totals,\n",
        "                                      context_counts, context_totals,\n",
        "                                      all_tags)\n",
        "\n",
        "\n",
        "our_tags = re.findall(r'(\\S+){(\\S+)=(\\S+)}', our)\n",
        "pymorphy_tags = analyze_with_pymorphy(sample_text)\n",
        "\n",
        "acc = evaluate_accuracy(our_tags, pymorphy_tags)\n",
        "print(f\"Совпадение по частям речи: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU0HT6S6td_k",
        "outputId": "47b67926-b1f1-4d8b-992c-e6575109a0d8"
      },
      "id": "mU0HT6S6td_k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Совпадение по частям речи: 0.739\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}